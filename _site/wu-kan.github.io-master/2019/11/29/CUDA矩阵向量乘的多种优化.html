<h2 id="实验简介">实验简介</h2>

<p>使用下面一种或多种优化方法完成 CUDA 的矩阵向量乘法$y=A\times x$,其中$A$是$2^{14}\times 2^{14}$的方阵，$x$为$2^{14}$维向量。假设矩阵$A$的元素为$a_{i,j}=i-0.1\times j+1$，向量$x$的元素为$b_i=\log\sqrt{i\times i-i+2}$。</p>

<ul>
  <li>使用 global memory</li>
  <li>使用合并访存</li>
  <li>使用 constant memory 存放向量</li>
  <li>使用 shared memory 存放向量和矩阵</li>
  <li>使用 warp 直接访问寄存器</li>
  <li>使用 <code class="highlighter-rouge">cublasSgemv</code></li>
</ul>

<h2 id="实验环境">实验环境</h2>

<p>实验在老师提供的计算集群的一个节点上进行。单节点的显卡配置如下：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvdia-smi
Mon Dec  2 08:38:49 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|<span class="o">=============================================================================</span>|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div>

<h2 id="实验原理">实验原理</h2>

<p>优化 CUDA 架构上的程序，一般从以下几个方面考虑：</p>

<ul>
  <li>选择好的并行算法，发掘更多的数据并行性</li>
  <li>保持 SM 尽可能忙碌，尽量利用所有的 SM 参与计算
    <ul>
      <li>加大数据量</li>
      <li>减小线程块大小</li>
    </ul>
  </li>
  <li>优化存储器的使用
    <ul>
      <li>全局存储器合并访问</li>
      <li>使用更快的 constant memory 或 shared memory</li>
    </ul>
  </li>
  <li>使用一些已有的库，如<code class="highlighter-rouge">&lt;cuBlas_v2.h&gt;</code></li>
</ul>

<h2 id="实验过程">实验过程</h2>

<p>由于都是 CUDA 架构上的核函数对比性能，下面的计时都只测了用于核函数计算的时间，而不包含数据拷贝的部分（否则运行时间都在 300ms 左右，基本上都是拷贝的时间而没有参考价值了）。当然，由于没有计入拷贝等预处理的时间，那些需要计算转置（列优先）或者预读取的算法在这里会有优势一些。</p>

<h3 id="使用-global-memory">使用 global memory</h3>

<p>这是最基础的矩阵向量乘法。这里假设线程块都是一维组织的，每个 CUDA 线程计算矩阵的一行与向量乘积，这样各线程之间没有读写冲突，不需要使用原子操作。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvGlobalMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ar</span><span class="p">,</span> <span class="c1">//行优先形式，下同</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span> <span class="c1">//A的行数</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="c1">//A的列数</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//将结果先存在寄存器里，减少对向量y的访存</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ar</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间为<code class="highlighter-rouge">4.694240ms</code>。</p>

<h3 id="使用合并访存">使用合并访存</h3>

<p>所谓合并访存，指的是相邻的线程访问段对齐的地址。比如在之前的代码中，<code class="highlighter-rouge">j == 0</code>时线程 0 访问<code class="highlighter-rouge">Ar[0]</code>，线程 1 访问<code class="highlighter-rouge">Ar[nCol]</code>，线程 2 访问<code class="highlighter-rouge">Ar[2 * nCol]</code>…它们并不相邻，因此不满足合并访问的要求。在这里我们把原来的行优先矩阵$A$转换成列优先表示形式（即行优先下的转置$A^T$），此时<code class="highlighter-rouge">j == 0</code>时线程 0 访问<code class="highlighter-rouge">Ac[0]</code>，线程 1 访问<code class="highlighter-rouge">Ac[1]</code>，线程 2 访问<code class="highlighter-rouge">Ac[2]</code>…此时满足了合并访问的要求。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvGlobalMemoryAlign</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span> <span class="c1">//列优先形式，下同</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间为<code class="highlighter-rouge">1.551584ms</code>，性能提高了将近三倍，充分说明了合并访存的重要性。</p>

<h3 id="使用-constant-memory-存放向量">使用 constant memory 存放向量</h3>

<p>注意到向量在计算过程中不会改变，且每个线程访问相同地址，因此考虑把它放在 constant memory 中。</p>

<p>NVIDIA 硬件提供了 64KB 的常量内存，并且常量内存采用了不同于标准全局内存的处理方式。在这里我们大小为$2^{14}$的单精度浮点数向量$x$大小恰好为 64KB，正好可以完整保存。如果向量超过了 constant memory 的 64KB 上限，那就需要分批进行，多次传输和启动内核。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">__constant__</span> <span class="n">d_cx</span><span class="p">[(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">16</span><span class="p">)</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)];</span> <span class="c1">//64KB</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvConstantMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">d_cx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间为<code class="highlighter-rouge">1.516992ms</code>，在上一步的基础上略微提高。使用常量内存可以提升运算性能的原因主要有两个：</p>

<ol>
  <li>对常量内存的单次读操作可以广播到同个半线程束的其他$15$个线程，这种方式产生的内存流量只是使用全局内存时的$\frac{1}{16}$。</li>
  <li>硬件将主动把常量数据缓存在 GPU 上。在第一次从常量内存的某个地址上读取后，当其他半线程束请求同一个地址时，那么将命中缓存，这同样减少了额外的内存流量。</li>
</ol>

<h3 id="使用-shared-memory-存放向量和矩阵">使用 shared memory 存放向量和矩阵</h3>

<p>对于 block 内内存来说，向量都是共享的，因此我们可以使用比 constant memory 更快的 shared memory 来存储，此时相比较使用常量内存，我们免掉了向量比较大的时候多次数据拷贝和启动核函数的开销，也没有使用全局变量，增加了代码的可扩展性。当然，shared memory 更小（48K），因此需要对向量进行分块处理。</p>

<p>另外需要更正的一个问题是，并不需要使用 shared memory 去存矩阵，因为在这个矩阵向量乘的过程中，每个矩阵元素只被访问了一次。此外，shared memory 的大小也并不足以存下完整的矩阵（甚至是向量）。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">reduce_size</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvSharedMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">extern</span> <span class="kt">float</span> <span class="n">__shared__</span> <span class="n">sx</span><span class="p">[];</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">jBegLast</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">reduce_size</span> <span class="o">*</span> <span class="n">reduce_size</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">jBeg</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">&lt;</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">+=</span> <span class="n">reduce_size</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">//防止有的进程还在读sx</span>
		<span class="n">sx</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBeg</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
			<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">reduce_size</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//能够自动展开</span>
				<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBeg</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">//防止有的进程还在读sx</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
			<span class="n">sx</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
			<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">-</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//不能自动展开</span>
				<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBegLast</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间为<code class="highlighter-rouge">1.400672ms</code>。注意这里我们将循环展开了，好处是减少了核函数运行时的分支。如果不展开的话，其运行时间将退化到比之前的还慢。</p>

<h4 id="使用-shuffle">使用 shuffle</h4>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">warp_size</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvWarp</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">lane_id</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">warp_size</span><span class="p">,</span>
		<span class="n">jBegLast</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">warp_size</span> <span class="o">*</span> <span class="n">warp_size</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">jBeg</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">&lt;</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">+=</span> <span class="n">warp_size</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="k">const</span> <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBeg</span> <span class="o">+</span> <span class="n">lane_id</span><span class="p">];</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">warp_size</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//能够自动展开</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBeg</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">warp_size</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="p">{</span>
		<span class="k">const</span> <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">jBegLast</span> <span class="o">+</span> <span class="n">lane_id</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">?</span> <span class="n">x</span><span class="p">[</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">lane_id</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">-</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//不能自动展开</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBegLast</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">warp_size</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间<code class="highlighter-rouge">1.594368ms</code>，反而有一定的下降。分析一下原因：老师集群上的显卡性能过于强悍（在今年十一月 SC 超算大会刚发布 Tesla V100S 前，Tesla V100 一直都是市面能买到的最强算力），内存读写性能比以往的显卡都要强很多，因此对本来已经很快的 shared memory 的优化效果没有那么明显了，而由于<code class="highlighter-rouge">warp_size</code>小于<code class="highlighter-rouge">reduce_size</code>导致循环分支次数却比上一步多，效果变差。</p>

<h3 id="使用-cublas_v2h">使用 <code class="highlighter-rouge">&lt;cublas_v2.h&gt;</code></h3>

<p>该函数的官方文档见<a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemv">https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemv</a>。下面是传入行优先矩阵的调用。要尤其注意的是，cublas库为了与FORTRAN中的接口保持一致，默认的稠密矩阵是按照列优先方法存储的，因此对于行优先存储的形式反而要标记转置。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cublasSgemv</span><span class="p">(</span>
	<span class="n">wk_cublas_handle</span><span class="p">,</span>
	<span class="n">CUBLAS_OP_T</span><span class="p">,</span>
	<span class="n">m</span><span class="p">,</span>
	<span class="n">n</span><span class="p">,</span>
	<span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span>
	<span class="n">d_Ar</span><span class="p">,</span>
	<span class="n">m</span><span class="p">,</span>
	<span class="n">d_x</span><span class="p">,</span>
	<span class="mi">1</span><span class="p">,</span>
	<span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span>
	<span class="n">d_y</span><span class="p">,</span>
	<span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>

<p>运行时间<code class="highlighter-rouge">1.863520ms</code>。下面是传入列优先矩阵的调用。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cublasSgemv</span><span class="p">(</span>
	<span class="n">wk_cublas_handle</span><span class="p">,</span>
	<span class="n">CUBLAS_OP_N</span><span class="p">,</span>
	<span class="n">m</span><span class="p">,</span>
	<span class="n">n</span><span class="p">,</span>
	<span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span>
	<span class="n">d_Ac</span><span class="p">,</span>
	<span class="n">m</span><span class="p">,</span>
	<span class="n">d_x</span><span class="p">,</span>
	<span class="mi">1</span><span class="p">,</span>
	<span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span>
	<span class="n">d_y</span><span class="p">,</span>
	<span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>

<p>运行时间<code class="highlighter-rouge">1.381888ms</code>。（艹，我做了这半天的优化结果还没它快）</p>

<h3 id="sgemvo12727"><code class="highlighter-rouge">Sgemv.o12727</code></h3>

<p>分别是上面几种方法函数的运行时间。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>4.694240ms
1.551584ms
1.516992ms
1.400672ms
1.594368ms
1.863520ms
1.381888ms
</code></pre></div></div>

<p>可以看到，由于现在的硬件性能已经大大强于数年前，做存储器的优化效果已经比较小（并不是说没有，只是甚至已经小于多次循环跳转的开销了）。因此，对这个问题来说，最主要的是要选一个优秀的并行算法，再对程序代码做好访存分析和优化。</p>

<p>当然也不是说存储器结构就不再重要，还是要具体问题具体分析。上面很多算法都是要对矩阵或者向量进行预处理的，而并没有把对应的代价（时间、内存空间、可扩展性等）计入在内，实际上在运用到生产环境的时候这些仍然是必须要考虑的。最后，虽然前面的代码还没有调库跑得快，实际上还是有优化余地的，比如：</p>

<ul>
  <li>使用<code class="highlighter-rouge">#pragma unroll</code>进行循环展开。
    <ul>
      <li>我在测试的时候用这种方法一度将<code class="highlighter-rouge">wkSgemvConstantMemory</code>优化至<code class="highlighter-rouge">1.35ms</code>，但是这种方法实际上是面向硬件和数据本身的（需要提前知道有多少数据被优化，才能确定展开多少次是安全的），在真正的生产环境中可移植性不够高，我不是非常喜欢。</li>
    </ul>
  </li>
  <li>多路启动，每个线程负责多个向量元素，可以减少调度时开销。
    <ul>
      <li>不喜欢的理由同上。</li>
    </ul>
  </li>
</ul>

<p>总之结论就是，在实际生产力环境中还是直接调库来的省事，库中的黑科技代码在绝大多数情况下都是要优秀于自己写的代码的。当然，自己手动做过这个实验也可以帮助我们对库的代码做进了一步的理解，比如列优先矩阵的向量乘法为什么优秀于行优先。</p>

<h3 id="sgemvpbs"><code class="highlighter-rouge">Sgemv.pbs</code></h3>

<p>调度脚本。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#PBS -N Sgemv</span>
<span class="c">#PBS -l nodes=1:ppn=32:gpus=1</span>
<span class="c">#PBS -j oe</span>
<span class="c">#PBS -q gpu</span>
<span class="nb">source</span> /public/software/profile.d/cuda10.0.sh
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
nvcc Sgemv.cu <span class="nt">-run</span> <span class="nt">-lcublas</span>
</code></pre></div></div>

<h3 id="sgemvcu"><code class="highlighter-rouge">Sgemv.cu</code></h3>

<p>完整代码。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;stdio.h&gt;
#include &lt;math.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;cublas_v2.h&gt;
</span><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvGlobalMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ar</span><span class="p">,</span> <span class="c1">//行优先形式，下同</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span> <span class="c1">//A的行数</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="c1">//A的列数</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//将结果先存在寄存器里，减少对向量y的访存</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ar</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvGlobalMemoryAlign</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span> <span class="c1">//列优先形式，下同</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
<span class="kt">float</span> <span class="n">__constant__</span> <span class="n">d_cx</span><span class="p">[(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">16</span><span class="p">)</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)];</span> <span class="c1">//64KB</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvConstantMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">d_cx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">reduce_size</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvSharedMemory</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">extern</span> <span class="kt">float</span> <span class="n">__shared__</span> <span class="n">sx</span><span class="p">[];</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">jBegLast</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">reduce_size</span> <span class="o">*</span> <span class="n">reduce_size</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">jBeg</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">&lt;</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">+=</span> <span class="n">reduce_size</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">//防止有的进程还在读sx</span>
		<span class="n">sx</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBeg</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
			<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">reduce_size</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//能够自动展开</span>
				<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBeg</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">//防止有的进程还在读sx</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
			<span class="n">sx</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
			<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">-</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//不能自动展开</span>
				<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBegLast</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sx</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">warp_size</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">wkSgemvWarp</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">Ac</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span>
	<span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">m</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
		<span class="n">lane_id</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">warp_size</span><span class="p">,</span>
		<span class="n">jBegLast</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">warp_size</span> <span class="o">*</span> <span class="n">warp_size</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">jBeg</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">&lt;</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="n">jBeg</span> <span class="o">+=</span> <span class="n">warp_size</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="k">const</span> <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">jBeg</span> <span class="o">+</span> <span class="n">lane_id</span><span class="p">];</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">warp_size</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//能够自动展开</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBeg</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">warp_size</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="p">{</span>
		<span class="k">const</span> <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">jBegLast</span> <span class="o">+</span> <span class="n">lane_id</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">?</span> <span class="n">x</span><span class="p">[</span><span class="n">jBegLast</span> <span class="o">+</span> <span class="n">lane_id</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">-</span> <span class="n">jBegLast</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="c1">//不能自动展开</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">Ac</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">jBegLast</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">warp_size</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
		<span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">m</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">14</span><span class="p">,</span>
		<span class="n">n</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">14</span><span class="p">;</span>
	<span class="kt">float</span>
		<span class="o">*</span><span class="n">h_Ar</span><span class="p">,</span>
		<span class="o">*</span><span class="n">h_Ac</span><span class="p">,</span>
		<span class="o">*</span><span class="n">h_x</span><span class="p">,</span>
		<span class="o">*</span><span class="n">d_Ar</span><span class="p">,</span>
		<span class="o">*</span><span class="n">d_Ac</span><span class="p">,</span>
		<span class="o">*</span><span class="n">d_x</span><span class="p">,</span>
		<span class="o">*</span><span class="n">d_y</span><span class="p">;</span>
	<span class="n">cudaHostAlloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">h_Ar</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaHostAllocWriteCombined</span><span class="p">);</span>
	<span class="n">cudaHostAlloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">h_Ac</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaHostAllocWriteCombined</span><span class="p">);</span>
	<span class="n">cudaHostAlloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">h_x</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaHostAllocWriteCombined</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">h_x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">j</span> <span class="o">*</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span><span class="p">));</span>
		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
			<span class="n">h_Ac</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_Ar</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">cudaMalloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">float</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_Ar</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">);</span>
	<span class="n">cudaMalloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">float</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_Ac</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">);</span>
	<span class="n">cudaMalloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">float</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_x</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">);</span>
	<span class="n">cudaMalloc</span><span class="p">(</span>
		<span class="p">(</span><span class="kt">float</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_y</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span><span class="p">);</span>
	<span class="n">cudaMemcpy</span><span class="p">(</span>
		<span class="n">d_Ar</span><span class="p">,</span>
		<span class="n">h_Ar</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
	<span class="n">cudaMemcpy</span><span class="p">(</span>
		<span class="n">d_Ac</span><span class="p">,</span>
		<span class="n">h_Ac</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
	<span class="n">cudaMemcpy</span><span class="p">(</span>
		<span class="n">d_x</span><span class="p">,</span>
		<span class="n">h_x</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
	<span class="n">cudaMemcpyToSymbol</span><span class="p">(</span>
		<span class="n">d_cx</span><span class="p">,</span>
		<span class="n">h_x</span><span class="p">,</span>
		<span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span>
		<span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
	<span class="n">cudaFreeHost</span><span class="p">(</span><span class="n">h_Ar</span><span class="p">);</span>
	<span class="n">cudaFreeHost</span><span class="p">(</span><span class="n">h_Ac</span><span class="p">);</span>
	<span class="n">cudaFreeHost</span><span class="p">(</span><span class="n">h_x</span><span class="p">);</span>
	<span class="n">cublasHandle_t</span> <span class="n">wk_cublas_handle</span><span class="p">;</span>
	<span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">wk_cublas_handle</span><span class="p">);</span>
	<span class="kt">float</span>
		<span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
		<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">cudaEvent_t</span> <span class="n">beg</span><span class="p">,</span> <span class="n">end</span><span class="p">;</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">end</span><span class="p">);</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">beg</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="k">const</span> <span class="kt">size_t</span>
			<span class="n">blocks</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">7</span><span class="p">,</span>
			<span class="n">grids</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blocks</span><span class="p">;</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">wkSgemvGlobalMemory</span><span class="o">&lt;&lt;&lt;</span><span class="n">grids</span><span class="p">,</span> <span class="n">blocks</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">d_Ar</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
			<span class="n">wkSgemvGlobalMemoryAlign</span><span class="o">&lt;&lt;&lt;</span><span class="n">grids</span><span class="p">,</span> <span class="n">blocks</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">d_Ac</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
			<span class="n">wkSgemvConstantMemory</span><span class="o">&lt;&lt;&lt;</span><span class="n">grids</span><span class="p">,</span> <span class="n">blocks</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">d_Ac</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span>
			<span class="n">wkSgemvSharedMemory</span><span class="o">&lt;</span><span class="n">blocks</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">grids</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">blocks</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">d_Ac</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span>
			<span class="n">wkSgemvWarp</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">grids</span><span class="p">,</span> <span class="n">blocks</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">d_Ac</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">)</span>
			<span class="n">cublasSgemv</span><span class="p">(</span>
				<span class="n">wk_cublas_handle</span><span class="p">,</span>
				<span class="n">CUBLAS_OP_T</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span>
				<span class="n">d_Ar</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="mi">1</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="mi">1</span><span class="p">);</span>
		<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">)</span>
			<span class="n">cublasSgemv</span><span class="p">(</span>
				<span class="n">wk_cublas_handle</span><span class="p">,</span>
				<span class="n">CUBLAS_OP_N</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span>
				<span class="n">d_Ac</span><span class="p">,</span>
				<span class="n">m</span><span class="p">,</span>
				<span class="n">d_x</span><span class="p">,</span>
				<span class="mi">1</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span>
				<span class="n">d_y</span><span class="p">,</span>
				<span class="mi">1</span><span class="p">);</span>
		<span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">end</span><span class="p">);</span>
		<span class="kt">float</span> <span class="n">elapsed_time</span><span class="p">;</span>
		<span class="n">cudaEventElapsedTime</span><span class="p">(</span><span class="o">&amp;</span><span class="n">elapsed_time</span><span class="p">,</span> <span class="n">beg</span><span class="p">,</span> <span class="n">end</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">"%fms</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">cublasDestroy</span><span class="p">(</span><span class="n">wk_cublas_handle</span><span class="p">);</span>
	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_Ar</span><span class="p">);</span>
	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_Ac</span><span class="p">);</span>
	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_x</span><span class="p">);</span>
	<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_y</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>
