<h2 id="简介">简介</h2>

<p>上次我学习了<a href="https://wu-kan.cn/_posts/2020-02-25-%E7%94%A8Shuffle%E5%8A%A0%E9%80%9FCUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/">用 Shuffle 加速 CUDA 上的 Reduce 操作</a>，据说这是目前在 CUDA 上最快的区间规约算法。然而运用在实际的情况中却并没有对代码的性能带来多大提升。本文中我再次整理了自己已知的所有 CUDA 上的快速区间规约方法，并以此对写出<strong>高性能</strong>且<strong>高可扩展</strong>的 CUDA 代码提出一些自己的思考。</p>

<ul>
  <li>多路规约</li>
  <li>使用 Shared Memory 和 Warp Shuffle 增加计算带宽</li>
  <li>使用 <code class="highlighter-rouge">&lt;thrust/reduce.h&gt;</code></li>
  <li>使用 <code class="highlighter-rouge">&lt;cublas_v2.h&gt;</code></li>
</ul>

<h2 id="实验环境">实验环境</h2>

<p>使用 v100 集群上一个结点的单张 v100 运行。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvdia-smi
Mon Dec  2 08:38:49 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|<span class="o">=============================================================================</span>|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div>

<h2 id="实验过程与分析">实验过程与分析</h2>

<h3 id="一些说明">一些说明</h3>

<p>和前一个实验<a href="https://wu-kan.cn/_posts/2020-02-25-%E7%94%A8Shuffle%E5%8A%A0%E9%80%9FCUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/">用 Shuffle 加速 CUDA 上的 Reduce 操作</a>不同，这里被规约元素的类型不再是 <code class="highlighter-rouge">unsigned</code> 而是 <code class="highlighter-rouge">double</code>，更加贴合实际使用场景。单个 <code class="highlighter-rouge">unsigned</code> 内存占用 4 字节，而 <code class="highlighter-rouge">double</code> 是 8 字节，这就使得 Warp 间互相访问寄存器的流量增加了一倍，很大程度上降低了 Shuffle 的优化效果（推测）。</p>

<h3 id="thrustreduce"><code class="highlighter-rouge">thrust::reduce</code></h3>

<p>首先是性能标杆 <code class="highlighter-rouge">thrust</code> 库，来看一看目前最流行的实现可以达到怎样的性能。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">n</span><span class="p">);</span>
</code></pre></div></div>

<p>运行时间 <code class="highlighter-rouge">10.561248ms</code>。</p>

<h3 id="simpledasum"><code class="highlighter-rouge">simpleDasum</code></h3>

<p>先来做最基础的算法优化，几乎无需掌握任何 CUDA 内存分布的知识。</p>

<p>首先，由于这里规约的元素数量高达十亿个，如果按照通常习惯的每个线程对应输入的一个元素，那么显卡的调度开销一定程度上无法忽视。这里我们让一个线程可以对应输入中的多个元素，减少所需要的线程数量，从而减少调度开销。</p>

<p>最后，我们用 <code class="highlighter-rouge">template &lt;size_t UNROLL_SIZE&gt;</code> 传入 <code class="highlighter-rouge">#pragma unroll(UNROLL_SIZE)</code> 循环展开次数，这样编译器可以在代码生成的时候将循环展开，以减少多次循环跳转的开销。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">UNROLL_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">simpleDasumKernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">double</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">double</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#pragma unroll(UNROLL_SIZE)
</span>	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">global_id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
		<span class="n">val</span> <span class="o">+=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
	<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间为<code class="highlighter-rouge">10.727680ms</code>，和<code class="highlighter-rouge">thrust</code>相比还算可以接受。</p>

<h3 id="naivedasum"><code class="highlighter-rouge">naiveDasum</code></h3>

<p>在 <code class="highlighter-rouge">simpleDasum</code> 基础上，优化存储器的使用。</p>

<p>对于同一个 block 内的所有线程，我们可以借助 Shared Memory 再进行一次树形规约，从而减少对内存的写操作和二次规约。</p>

<p>对于同一个 warp 内的所有线程也是同理，warp 间直接访问寄存器的开销比 Shared Memory 更小。此外 warp 同步的开销也要小于<code class="highlighter-rouge">__syncthreads()</code>，并且同一个 warp 上执行语句是不需要条件分支的，因为不管怎样都会被执行。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span>
	<span class="kt">size_t</span> <span class="n">UNROLL_SIZE</span><span class="p">,</span>
	<span class="kt">size_t</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
	<span class="kt">size_t</span> <span class="n">WARP_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">naiveDasumKernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">double</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">double</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">__shared__</span> <span class="n">shared</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
	<span class="p">{</span>
		<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#pragma unroll(UNROLL_SIZE)
</span>		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">global_id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
			<span class="n">val</span> <span class="o">+=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
		<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
	<span class="p">}</span>
<span class="cp">#pragma unroll
</span>	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">);</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span>
			<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">^</span> <span class="n">offset</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">WARP_SIZE</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="cp">#pragma unroll
</span>		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
			<span class="n">val</span> <span class="o">+=</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">tmp_d</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>运行时间 <code class="highlighter-rouge">10.467008ms</code> ，终于比 <code class="highlighter-rouge">thrust::reduce</code> 快了一丢丢。</p>

<h3 id="cublasdasum"><code class="highlighter-rouge">cublasDasum</code></h3>

<p>也可以使用线性代数库 <code class="highlighter-rouge">&lt;cublas_v2.h&gt;</code> 中的 <code class="highlighter-rouge">asum</code> 系列函数实现。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cublasDasum</span><span class="p">(</span>
	<span class="n">wk_cublas_handle</span><span class="p">,</span>
	<span class="n">n</span><span class="p">,</span>
	<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
	<span class="mi">1</span><span class="p">,</span>
	<span class="o">&amp;</span><span class="n">sum</span><span class="p">);</span>
</code></pre></div></div>

<p>运行时间为 <code class="highlighter-rouge">11.918208ms</code>，看来<code class="highlighter-rouge">cublas</code>库提供的线性代数抽象有一定的开销。</p>

<h2 id="总结">总结</h2>

<p>可以看到，随着硬件技术的发展，显卡上的运行速度已经是相当快了，十亿多的数据只用了十毫秒就完成了规约，并且在这种前提下存储优化的效果越来越有限了。</p>

<p>然而，较之略显复杂的访存优化，一些简单的编程习惯反而能有效提高 CUDA 代码的效率。从可扩展性的角度来说，我也更倾向于写 <code class="highlighter-rouge">simpleDasum</code> 这样对硬件的依赖程度更低的代码。毕竟未来显卡到底会怎么发展谁也说不准，也许以后一个 warp 或者一个 block 中会有更多的线程。</p>

<p>最后调库大法好，自己做了半天优化最后也只比库快了一丢丢，从开发成本的角度来说还是不要重复造轮子为妙。</p>

<h2 id="源代码">源代码</h2>

<h3 id="dasumpbs"><code class="highlighter-rouge">Dasum.pbs</code></h3>

<p>调度脚本。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#PBS -N Dasum</span>
<span class="c">#PBS -l nodes=1:ppn=32:gpus=1</span>
<span class="c">#PBS -j oe</span>
<span class="c">#PBS -q gpu</span>
<span class="nb">source</span> /public/software/profile.d/cuda10.0.sh
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
nvcc Dasum.cu <span class="nt">-run</span> <span class="nt">-lcublas</span>
</code></pre></div></div>

<h3 id="dasumo18880"><code class="highlighter-rouge">Dasum.o18880</code></h3>

<p>运行结果。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1073741824.000000 : 10.561248ms elapsed.
1073741824.000000 : 10.727680ms elapsed.
1073741824.000000 : 10.467008ms elapsed.
1073741824.000000 : 11.918208ms elapsed.
</code></pre></div></div>

<h3 id="dasumcu"><code class="highlighter-rouge">Dasum.cu</code></h3>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/reduce.h&gt;
#include &lt;cublas_v2.h&gt;
</span><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">UNROLL_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">simpleDasumKernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">double</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">double</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#pragma unroll(UNROLL_SIZE)
</span>	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">global_id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
		<span class="n">val</span> <span class="o">+=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
	<span class="n">tmp_d</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">template</span> <span class="o">&lt;</span>
	<span class="kt">size_t</span> <span class="n">UNROLL_SIZE</span><span class="p">,</span>
	<span class="kt">size_t</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
	<span class="kt">size_t</span> <span class="n">WARP_SIZE</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">__global__</span> <span class="nf">naiveDasumKernel</span><span class="p">(</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span>
	<span class="k">const</span> <span class="kt">double</span> <span class="o">*</span><span class="n">src_d</span><span class="p">,</span>
	<span class="kt">double</span> <span class="o">*</span><span class="n">tmp_d</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">__shared__</span> <span class="n">shared</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
	<span class="p">{</span>
		<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="cp">#pragma unroll(UNROLL_SIZE)
</span>		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">global_id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
			<span class="n">val</span> <span class="o">+=</span> <span class="n">src_d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
		<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
	<span class="p">}</span>
<span class="cp">#pragma unroll
</span>	<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">);</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span>
			<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">^</span> <span class="n">offset</span><span class="p">];</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">WARP_SIZE</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="cp">#pragma unroll
</span>		<span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
			<span class="n">val</span> <span class="o">+=</span> <span class="n">__shfl_xor_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">tmp_d</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="k">const</span> <span class="kt">size_t</span>
		<span class="n">n</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
		<span class="n">REDUCE_SIZE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">22</span><span class="p">,</span>
		<span class="n">UNROLL_SIZE</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">REDUCE_SIZE</span><span class="p">,</span>
		<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">10</span><span class="p">,</span>
		<span class="n">WARP_SIZE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">5</span><span class="p">;</span>
	<span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">src</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tmp</span><span class="p">(</span><span class="n">REDUCE_SIZE</span><span class="p">);</span>
	<span class="n">cublasHandle_t</span> <span class="n">wk_cublas_handle</span><span class="p">;</span>
	<span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">wk_cublas_handle</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">op</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="o">++</span><span class="n">op</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">double</span> <span class="n">sum</span><span class="p">;</span>
		<span class="n">cudaEvent_t</span> <span class="n">beg</span><span class="p">,</span> <span class="n">end</span><span class="p">;</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">end</span><span class="p">);</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">beg</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">src</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">n</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
		<span class="p">{</span>
			<span class="n">simpleDasumKernel</span><span class="o">&lt;</span>
				<span class="n">UNROLL_SIZE</span><span class="o">&gt;&lt;&lt;&lt;</span>
				<span class="p">(</span><span class="n">REDUCE_SIZE</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
				<span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">data</span><span class="p">()));</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">REDUCE_SIZE</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
		<span class="p">{</span>
			<span class="n">naiveDasumKernel</span><span class="o">&lt;</span>
				<span class="n">UNROLL_SIZE</span><span class="p">,</span>
				<span class="n">BLOCK_SIZE</span><span class="p">,</span>
				<span class="n">WARP_SIZE</span><span class="o">&gt;&lt;&lt;&lt;</span>
				<span class="p">(</span><span class="n">REDUCE_SIZE</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
				<span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">data</span><span class="p">()));</span>
			<span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">tmp</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">REDUCE_SIZE</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span>
			<span class="n">cublasDasum</span><span class="p">(</span>
				<span class="n">wk_cublas_handle</span><span class="p">,</span>
				<span class="n">n</span><span class="p">,</span>
				<span class="n">thrust</span><span class="o">::</span><span class="n">raw_pointer_cast</span><span class="p">(</span><span class="n">src</span><span class="p">.</span><span class="n">data</span><span class="p">()),</span>
				<span class="mi">1</span><span class="p">,</span>
				<span class="o">&amp;</span><span class="n">sum</span><span class="p">);</span>
		<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">beg</span><span class="p">);</span>
		<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">end</span><span class="p">);</span>
		<span class="kt">float</span> <span class="n">elapsed_time</span><span class="p">;</span>
		<span class="n">cudaEventElapsedTime</span><span class="p">(</span>
			<span class="o">&amp;</span><span class="n">elapsed_time</span><span class="p">,</span>
			<span class="n">beg</span><span class="p">,</span>
			<span class="n">end</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">"%f : %fms elapsed.</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">sum</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">);</span>
	<span class="p">}</span>
	<span class="n">cublasDestroy</span><span class="p">(</span><span class="n">wk_cublas_handle</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>
