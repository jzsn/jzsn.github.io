I"cU<h2 id="reference-materials">Reference Materials</h2>

<ul>
  <li>Stanford: <strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong> by Fei-Fei Li,etc.
    <ul>
      <li>Course website: <a href="http://cs231n.stanford.edu/2017/syllabus.html">http://cs231n.stanford.edu/2017/syllabus.html</a></li>
      <li>Video website: <a href="https://www.bilibili.com/video/av17204303/?p=9&amp;tdsourcetag=s_pctim_aiomsg">https://www.bilibili.com/video/av17204303/?p=9&amp;tdsourcetag=s_pctim_aiomsg</a></li>
    </ul>
  </li>
  <li><strong>Machine Learning</strong> by Hung-yi Lee
    <ul>
      <li>Course website: <a href="http://speech.ee.ntu.edu.tw/~tlkagk/index.html">http://speech.ee.ntu.edu.tw/~tlkagk/index.html</a></li>
      <li>Video website: <a href="https://www.bilibili.com/video/av9770302/from=search">https://www.bilibili.com/video/av9770302/from=search</a></li>
    </ul>
  </li>
</ul>

<p>A Simple neural network code template</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*
</span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># Shorthand:
# "pd_" as a variable prefix means "partial derivative"
# "d_" as a variable prefix means "derivative"
# "_wrt_" is shorthand for "with respect to"
# "w_ho" and "w_ih" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively
</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">hidden_layer_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">hidden_layer_bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output_layer_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output_layer_bias</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
    <span class="k">def</span> <span class="nf">init_weights_from_inputs_to_hidden_layer_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_layer_weights</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
    <span class="k">def</span> <span class="nf">init_weights_from_hidden_layer_neurons_to_output_layer_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_layer_weights</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
    <span class="k">def</span> <span class="nf">inspect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'* Inputs: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Hidden Layer'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">inspect</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'* Output Layer'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">inspect</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># Uses online learning, ie updating the weights after each training case
</span>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_inputs</span><span class="p">,</span> <span class="n">training_outputs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">training_inputs</span><span class="p">)</span>

        <span class="c1"># 1. Output neuron deltas
</span>        <span class="c1"># Your Code Here
</span>        <span class="c1"># ∂E/∂zⱼ
</span>
        <span class="c1"># 2. Hidden neuron deltas
</span>        <span class="c1"># We need to calculate the derivative of the error with respect to the output of each hidden layer neuron
</span>        <span class="c1"># dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ
</span>        <span class="c1"># ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂
</span>        <span class="c1"># Your Code Here
</span>
        <span class="c1"># 3. Update output neuron weights
</span>        <span class="c1"># ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ
</span>        <span class="c1"># Δw = α * ∂Eⱼ/∂wᵢ
</span>        <span class="c1"># Your Code Here
</span>
        <span class="c1"># 4. Update hidden neuron weights
</span>        <span class="c1"># ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ
</span>        <span class="c1"># Δw = α * ∂Eⱼ/∂wᵢ
</span>        <span class="c1"># Your Code Here
</span>
    <span class="k">def</span> <span class="nf">calculate_total_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_sets</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="n">total_error</span>


<span class="k">class</span> <span class="nc">NeuronLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>

        <span class="c1"># Every neuron in a layer shares the same bias
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">Neuron</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">inspect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Neurons:'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">' Neuron'</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="n">weights</span><span class="p">)):</span>
                <span class="k">print</span><span class="p">(</span><span class="s">'  Weight:'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">w</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'  Bias:'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron</span><span class="p">.</span><span class="n">calculate_output</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">get_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">Neuron</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">calculate_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
    <span class="k">def</span> <span class="nf">calculate_total_net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># Apply the logistic function to squash the output of the neuron
</span>        <span class="c1"># The result is sometimes referred to as 'net' [2] or 'net' [1]
</span>    <span class="k">def</span> <span class="nf">squash</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_net_input</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># Determine how much the neuron's total input has to change to move closer to the expected output
</span>        <span class="c1">#
</span>        <span class="c1"># Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and
</span>        <span class="c1"># the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate
</span>        <span class="c1"># the partial derivative of the error with respect to the total net input.
</span>        <span class="c1"># This value is also known as the delta (δ) [1]
</span>        <span class="c1"># δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ
</span>        <span class="c1">#
</span>    <span class="k">def</span> <span class="nf">calculate_pd_error_wrt_total_net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># The error for each neuron is calculated by the Mean Square Error method:
</span>    <span class="k">def</span> <span class="nf">calculate_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># The partial derivate of the error with respect to actual output then is calculated by:
</span>        <span class="c1"># = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1
</span>        <span class="c1"># = -(target output - actual output)
</span>        <span class="c1">#
</span>        <span class="c1"># The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]
</span>        <span class="c1"># = actual output - target output
</span>        <span class="c1">#
</span>        <span class="c1"># Alternative, you can use (target - output), but then need to add it during backpropagation [3]
</span>        <span class="c1">#
</span>        <span class="c1"># Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:
</span>        <span class="c1"># = ∂E/∂yⱼ = -(tⱼ - yⱼ)
</span>    <span class="k">def</span> <span class="nf">calculate_pd_error_wrt_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># The total net input into the neuron is squashed using logistic function to calculate the neuron's output:
</span>        <span class="c1"># yⱼ = φ = 1 / (1 + e^(-zⱼ))
</span>        <span class="c1"># Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it
</span>        <span class="c1">#
</span>        <span class="c1"># The derivative (not partial derivative since there is only one variable) of the output then is:
</span>        <span class="c1"># dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)
</span>    <span class="k">def</span> <span class="nf">calculate_pd_total_net_input_wrt_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># The total net input is the weighted sum of all the inputs to the neuron and their respective weights:
</span>        <span class="c1"># = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...
</span>        <span class="c1">#
</span>        <span class="c1"># The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:
</span>        <span class="c1"># = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ
</span>    <span class="k">def</span> <span class="nf">calculate_pd_total_net_input_wrt_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>
        <span class="c1"># An example:
</span>

<span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_layer_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="n">hidden_layer_bias</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">output_layer_weights</span><span class="o">=</span><span class="p">[</span>
                   <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span> <span class="n">output_layer_bias</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">train</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">calculate_total_error</span><span class="p">([[[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]]]),</span> <span class="mi">9</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="horse-colic-data-set">Horse Colic Data Set</h2>

<p>The description of the horse colic data set (<a href="http://archive.ics.uci.edu/ml/datasets/Horse+Colic">http://archive.ics.uci.edu/ml/datasets/Horse+Colic</a>) is as follows:</p>

<p>We aim at trying to predict if a horse with colic will live or die.</p>

<p>Note that we should deal with missing values in the data! Here are some options:</p>

<ul>
  <li>Use the feature’s mean value from all the available data.</li>
  <li>Fill in the unknown with a special value like -1.</li>
  <li>Ignore the instance.</li>
  <li>Use a mean value from similar items.</li>
  <li>Use another machine learning algorithm to predict the value.</li>
</ul>

<h2 id="tasks">Tasks</h2>

<p>Given the training set <code class="highlighter-rouge">horse-colic.data</code> and the testing set <code class="highlighter-rouge">horse-colic.test</code>, implement the BP algorithm and establish a neural network to predict if horses with colic will live or die. In addition, you should calculate the accuracy rate.</p>

<h2 id="codes-and-results">Codes and Results</h2>

<h3 id="preprocesspy预处理"><code class="highlighter-rouge">preprocess.py</code>预处理</h3>

<p>感 谢 坤 哥 的 祝 福</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># coding=utf-8
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">np</span><span class="p">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>

<span class="c1"># 新建一个长度为len_vec的向量，除了第idx位为1外，其余位置的元素都是0
</span>

<span class="k">def</span> <span class="nf">onehot</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">len_vec</span><span class="p">):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">len_vec</span>
    <span class="n">vec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vec</span>


<span class="c1"># 初步处理训练集，把所有问号换成nan，其余不变
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'horse-colic.data'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fr</span><span class="p">:</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="p">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">splitted</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">strip</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">splitted</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'?'</span><span class="p">:</span>
                <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">train_set</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

<span class="c1"># 初步处理测试集，把所有问号换成nan，其余不变
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'horse-colic.test'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fr</span><span class="p">:</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="p">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">splitted</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">strip</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">splitted</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'?'</span><span class="p">:</span>
                <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">test_set</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>

<span class="c1"># DataFrame中的列名
</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'surgery'</span><span class="p">,</span> <span class="s">'age'</span><span class="p">,</span> <span class="s">'hospital number'</span><span class="p">,</span> <span class="s">'rectal temperature'</span><span class="p">,</span> <span class="s">'pulse'</span><span class="p">,</span> <span class="s">'respiratory rate'</span><span class="p">,</span> <span class="s">'temperature of extremities'</span><span class="p">,</span> <span class="s">'peripheral pulse'</span><span class="p">,</span> <span class="s">'mucous membranes'</span><span class="p">,</span> <span class="s">'capillary refill time'</span><span class="p">,</span> <span class="s">'pain'</span><span class="p">,</span> <span class="s">'peristalsis'</span><span class="p">,</span> <span class="s">'abdominal distension'</span><span class="p">,</span> <span class="s">'nasogastric tube'</span><span class="p">,</span>
           <span class="s">'nasogastric reflux'</span><span class="p">,</span> <span class="s">'nasogastric reflux PH'</span><span class="p">,</span> <span class="s">'rectal examination'</span><span class="p">,</span> <span class="s">'abdomen'</span><span class="p">,</span> <span class="s">'packed cell volume'</span><span class="p">,</span> <span class="s">'total protein'</span><span class="p">,</span> <span class="s">'abdominocentesis appearance'</span><span class="p">,</span> <span class="s">'abdomcentesis total protein'</span><span class="p">,</span> <span class="s">'outcome'</span><span class="p">,</span> <span class="s">'surgical lesion'</span><span class="p">,</span> <span class="s">'lesion type1'</span><span class="p">,</span> <span class="s">'lesion type2'</span><span class="p">,</span> <span class="s">'lesion type3'</span><span class="p">,</span> <span class="s">'cp_data'</span><span class="p">]</span>

<span class="c1"># 生成训练集的DataFrame
</span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="c1"># 生成测试集的DataFrame
</span><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="c1"># 将训练集与测试集纵向合并，方便两者一起进行预处理
</span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">])</span>

<span class="c1"># 删掉第3列，即'hospital number'这一列
</span><span class="n">df_train</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'hospital number'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 将第1列中的2都换成0
</span><span class="n">df_train</span><span class="p">.</span><span class="n">ix</span><span class="p">[</span><span class="n">df_train</span><span class="p">[</span><span class="s">'surgery'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'2'</span><span class="p">,</span> <span class="s">'surgery'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'0'</span>
<span class="c1"># 将第2列中的9都换成0
</span><span class="n">df_train</span><span class="p">.</span><span class="n">ix</span><span class="p">[</span><span class="n">df_train</span><span class="p">[</span><span class="s">'age'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'9'</span><span class="p">,</span> <span class="s">'age'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'0'</span>

<span class="c1"># 下面的for循环用于拆分原数据集第25、26、27这三列，比如将03111拆分成03、1、1、1
# 拆分的主要思想是先将这三列删掉，然后依次插入12列新数据
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'lesion type'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">().</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">series</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">new_cols</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">4</span><span class="p">]]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">series</span><span class="p">)])</span>
    <span class="n">df_train</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">df_train</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s">'site'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">new_cols</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">df_train</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'type'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">new_cols</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">df_train</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'subtype'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">new_cols</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">df_train</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'special code'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">new_cols</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">columns</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># 将训练集和测试集拆分
</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">300</span><span class="p">,</span> <span class="p">:],</span> <span class="n">df_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">300</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_set</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 计算训练集每一列的均值
</span><span class="n">average</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 将训练集中为nan的值替换为相应的均值
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">train_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]):</span>
            <span class="n">train_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">average</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

<span class="c1"># 将测试集中为nan的值替换为相应的均值
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_set</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_set</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">test_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]):</span>
            <span class="n">test_set</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">average</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

<span class="c1"># 保存训练集和测试集
</span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'horse-colic-data.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_test</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'horse-colic-test.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="bppy"><code class="highlighter-rouge">BP.py</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*
</span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># Shorthand:
# "pd_" as a variable prefix means "partial derivative"
# "d_" as a variable prefix means "derivative"
# "_wrt_" is shorthand for "with respect to"
# "w_ho" and "w_ih" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively
</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">hidden_layer_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">hidden_layer_bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output_layer_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output_layer_bias</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_inputs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">NeuronLayer</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">hidden_layer_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">NeuronLayer</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">output_layer_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">init_weights_from_inputs_to_hidden_layer_neurons</span><span class="p">(</span>
            <span class="n">hidden_layer_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">init_weights_from_hidden_layer_neurons_to_output_layer_neurons</span><span class="p">(</span>
            <span class="n">output_layer_weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights_from_inputs_to_hidden_layer_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_layer_weights</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">if</span> <span class="n">hidden_layer_weights</span><span class="p">:</span>
            <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span><span class="p">):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">hidden_layer_weights</span><span class="p">[</span><span class="n">cnt</span><span class="p">])</span>
                    <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span><span class="p">):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">init_weights_from_hidden_layer_neurons_to_output_layer_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_layer_weights</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">if</span> <span class="n">output_layer_weights</span><span class="p">:</span>
            <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">output_layer_weights</span><span class="p">[</span><span class="n">cnt</span><span class="p">])</span>
                    <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">inspect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'* Inputs: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Hidden Layer'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">inspect</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'* Output Layer'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">inspect</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'------'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

    <span class="c1"># Uses online learning, ie updating the weights after each training case
</span>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_inputs</span><span class="p">,</span> <span class="n">training_outputs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">training_inputs</span><span class="p">)</span>

        <span class="c1"># 1. Output neuron deltas
</span>        <span class="c1"># Your Code Here
</span>        <span class="n">pd_errors_wrt_output_neuron_total_net_input</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
            <span class="c1"># ∂E/∂zⱼ
</span>            <span class="n">pd_errors_wrt_output_neuron_total_net_input</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">calculate_pd_error_wrt_total_net_input</span><span class="p">(</span><span class="n">training_outputs</span><span class="p">[</span><span class="n">o</span><span class="p">]))</span>

        <span class="c1"># 2. Hidden neuron deltas
</span>        <span class="c1"># We need to calculate the derivative of the error with respect to the output of each hidden layer neuron
</span>        <span class="n">pd_errors_wrt_hidden_neuron_total_net_input</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>

            <span class="c1"># dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ
</span>            <span class="n">d_error_wrt_hidden_neuron_output</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
                <span class="n">d_error_wrt_hidden_neuron_output</span> <span class="o">+=</span> <span class="n">pd_errors_wrt_output_neuron_total_net_input</span><span class="p">[</span>
                    <span class="n">o</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>

        <span class="c1"># ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂
</span>        <span class="c1"># Your Code Here
</span>            <span class="n">pd_errors_wrt_hidden_neuron_total_net_input</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">d_error_wrt_hidden_neuron_output</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">calculate_pd_total_net_input_wrt_input</span><span class="p">())</span>

        <span class="c1"># 3. Update output neuron weights
</span>        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">w_ho</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">weights</span><span class="p">)):</span>
                <span class="c1"># ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ
</span>                <span class="n">pd_error_wrt_weight</span> <span class="o">=</span> <span class="n">pd_errors_wrt_output_neuron_total_net_input</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span>
                    <span class="n">o</span><span class="p">].</span><span class="n">calculate_pd_total_net_input_wrt_weight</span><span class="p">(</span><span class="n">w_ho</span><span class="p">)</span>

                <span class="c1"># Δw = α * ∂Eⱼ/∂wᵢ
</span>                <span class="c1"># Your Code Here
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">w_ho</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> \
                    <span class="n">pd_error_wrt_weight</span>

        <span class="c1"># 4. Update hidden neuron weights
</span>        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">w_ih</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">weights</span><span class="p">)):</span>
                <span class="c1"># ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ
</span>                <span class="n">pd_error_wrt_weight</span> <span class="o">=</span> <span class="n">pd_errors_wrt_hidden_neuron_total_net_input</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span>
                    <span class="n">h</span><span class="p">].</span><span class="n">calculate_pd_total_net_input_wrt_weight</span><span class="p">(</span><span class="n">w_ih</span><span class="p">)</span>

                <span class="c1"># Δw = α * ∂Eⱼ/∂wᵢ
</span>                <span class="c1"># Your Code Here
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">hidden_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">h</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">w_ih</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> \
                    <span class="n">pd_error_wrt_weight</span>

    <span class="k">def</span> <span class="nf">calculate_total_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_sets</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="n">total_error</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_sets</span><span class="p">)):</span>
            <span class="n">training_inputs</span><span class="p">,</span> <span class="n">training_outputs</span> <span class="o">=</span> <span class="n">training_sets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">training_inputs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_outputs</span><span class="p">)):</span>
                <span class="n">total_error</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">calculate_error</span><span class="p">(</span>
                    <span class="n">training_outputs</span><span class="p">[</span><span class="n">o</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">total_error</span>


<span class="k">class</span> <span class="nc">NeuronLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>

        <span class="c1"># Every neuron in a layer shares the same bias
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">Neuron</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">inspect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Neurons:'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">)):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">' Neuron'</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="n">weights</span><span class="p">)):</span>
                <span class="k">print</span><span class="p">(</span><span class="s">'  Weight:'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">n</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">w</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'  Bias:'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron</span><span class="p">.</span><span class="n">calculate_output</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">get_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">neurons</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">Neuron</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">calculate_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">squash</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">calculate_total_net_input</span><span class="p">())</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span>

    <span class="k">def</span> <span class="nf">calculate_total_net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">total</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

        <span class="c1"># Apply the logistic function to squash the output of the neuron
</span>        <span class="c1"># The result is sometimes referred to as 'net' [2] or 'net' [1]
</span>    <span class="k">def</span> <span class="nf">squash</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_net_input</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="c1"># return 1 / (1 + math.exp(-total_net_input))
</span>        <span class="k">return</span> <span class="n">math</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">total_net_input</span><span class="p">)</span>

        <span class="c1"># Determine how much the neuron's total input has to change to move closer to the expected output
</span>        <span class="c1">#
</span>        <span class="c1"># Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and
</span>        <span class="c1"># the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate
</span>        <span class="c1"># the partial derivative of the error with respect to the total net input.
</span>        <span class="c1"># This value is also known as the delta (δ) [1]
</span>        <span class="c1"># δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ
</span>        <span class="c1">#
</span>    <span class="k">def</span> <span class="nf">calculate_pd_error_wrt_total_net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">calculate_pd_error_wrt_output</span><span class="p">(</span><span class="n">target_output</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">calculate_pd_total_net_input_wrt_input</span><span class="p">()</span>

        <span class="c1"># The error for each neuron is calculated by the Mean Square Error method:
</span>    <span class="k">def</span> <span class="nf">calculate_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">target_output</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="c1"># The partial derivate of the error with respect to actual output then is calculated by:
</span>        <span class="c1"># = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1
</span>        <span class="c1"># = -(target output - actual output)
</span>        <span class="c1">#
</span>        <span class="c1"># The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]
</span>        <span class="c1"># = actual output - target output
</span>        <span class="c1">#
</span>        <span class="c1"># Alternative, you can use (target - output), but then need to add it during backpropagation [3]
</span>        <span class="c1">#
</span>        <span class="c1"># Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:
</span>        <span class="c1"># = ∂E/∂yⱼ = -(tⱼ - yⱼ)
</span>    <span class="k">def</span> <span class="nf">calculate_pd_error_wrt_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">target_output</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># The total net input into the neuron is squashed using logistic function to calculate the neuron's output:
</span>        <span class="c1"># yⱼ = φ = 1 / (1 + e^(-zⱼ))
</span>        <span class="c1"># Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it
</span>        <span class="c1">#
</span>        <span class="c1"># The derivative (not partial derivative since there is only one variable) of the output then is:
</span>        <span class="c1"># dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)
</span>    <span class="k">def</span> <span class="nf">calculate_pd_total_net_input_wrt_input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># The total net input is the weighted sum of all the inputs to the neuron and their respective weights:
</span>        <span class="c1"># = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...
</span>        <span class="c1">#
</span>        <span class="c1"># The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:
</span>        <span class="c1"># = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ
</span>    <span class="k">def</span> <span class="nf">calculate_pd_total_net_input_wrt_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="c1"># Your Code Here
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">getDataSets</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'outcome'</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'outcome'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
        <span class="n">mi</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">ma</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">mi</span> <span class="o">&gt;</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]:</span>
                <span class="n">mi</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">ma</span> <span class="o">&lt;</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]:</span>
                <span class="n">ma</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ma</span> <span class="o">!=</span> <span class="n">mi</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="n">mi</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">ma</span><span class="o">-</span><span class="n">mi</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]]</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="s">'''
    # An example:
    nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[
        0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)
    for i in range(10000):
        nn.train([0.05, 0.1], [0.01, 0.99])
        print(i, round(nn.calculate_total_error(
            [[[0.05, 0.1], [0.01, 0.99]]]), 9))
    '''</span>

    <span class="n">training_sets</span> <span class="o">=</span> <span class="n">getDataSets</span><span class="p">(</span><span class="s">"horse-colic-data.csv"</span><span class="p">)</span>
    <span class="n">testing_sets</span> <span class="o">=</span> <span class="n">getDataSets</span><span class="p">(</span><span class="s">"horse-colic-test.csv"</span><span class="p">)</span>

    <span class="n">nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_sets</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_sets</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">training_inputs</span><span class="p">,</span> <span class="n">training_outputs</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">training_sets</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_inputs</span><span class="p">,</span> <span class="n">training_outputs</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">calculate_total_error</span><span class="p">(</span><span class="n">testing_sets</span><span class="p">))</span>

    <span class="n">total_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testing_sets</span><span class="p">)):</span>
        <span class="n">testing_inputs</span><span class="p">,</span> <span class="n">testing_outputs</span> <span class="o">=</span> <span class="n">testing_sets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">testing_inputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">get_outputs</span><span class="p">()</span>
        <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testing_outputs</span><span class="p">)):</span>
            <span class="n">total_error</span> <span class="o">+=</span> <span class="n">nn</span><span class="p">.</span><span class="n">output_layer</span><span class="p">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">o</span><span class="p">].</span><span class="n">calculate_error</span><span class="p">(</span>
                <span class="n">testing_outputs</span><span class="p">[</span><span class="n">o</span><span class="p">])</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">testing_outputs</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">-</span><span class="n">outputs</span><span class="p">[</span><span class="n">o</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">cnt</span> <span class="o">=</span> <span class="n">cnt</span><span class="o">+</span><span class="mi">1</span>
        <span class="k">if</span> <span class="n">cnt</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">testing_outputs</span><span class="p">):</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span><span class="o">+</span><span class="mi">1</span>
    <span class="k">print</span><span class="p">(</span><span class="n">total_error</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">acc</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">testing_sets</span><span class="p">))</span>
</code></pre></div></div>
:ET